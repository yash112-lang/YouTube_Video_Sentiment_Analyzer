{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YouTube Video sentiment analyzer",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRHEe-q5R_ag"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import googleapiclient.discovery\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import json\n",
        "import urllib.request\n",
        "import random\n",
        "import pickle\n",
        "import random\n",
        "from urllib.parse import urlparse\n",
        "from urllib.parse import parse_qs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW-7JvoDxeRU",
        "outputId": "2bb97761-dc52-44e4-c3fd-4643cc8b6beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "0bceBVYCxr59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global vect\n",
        "vect = CountVectorizer()"
      ],
      "metadata": {
        "id": "GWUyRzPlpV1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_videoId(video_link=None):\n",
        "    if video_link is None:\n",
        "        print(\"Please provide a valid video Id.\")\n",
        "        print(\"Exiting ....\")\n",
        "        return None\n",
        "    url_data = urlparse(video_link)\n",
        "    video_id = parse_qs(url_data.query)['v'][0]\n",
        "    \n",
        "\n",
        "    return video_id"
      ],
      "metadata": {
        "id": "0BszJWa6X7nU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(line):\n",
        "    word_tokens = word_tokenize(line)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    return \" \".join(filtered_sentence)"
      ],
      "metadata": {
        "id": "yVjd_8kcy5Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_polarity(comments):\n",
        "    comments['polarity'] = comments['Comments'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "    comments = comments.sample(frac=1).reset_index(drop=True)\n",
        "    comments['pol_cat']  = 0\n",
        "    comments['pol_cat'][comments.polarity > 0] = 1\n",
        "    comments['pol_cat'][comments.polarity <= 0] = -1\n",
        "\n",
        "    # Converting comments in lowercase and removing leading and trailing extra spaces\n",
        "    comments['Comments'] = comments['Comments'].str.lower()\n",
        "    \n",
        "    # Removing stopwords\n",
        "    comments['stop_comments'] = comments['Comments'].apply(lambda x : remove_stopwords(x))\n",
        "    return comments"
      ],
      "metadata": {
        "id": "Nhc969GSuF3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataframe(response, video_id, training_data = False):\n",
        "  authorname = []\n",
        "  comments = []\n",
        "  for i in range(len(response[\"items\"])):\n",
        "    authorname.append(response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorDisplayName\"])\n",
        "    comments.append(response[\"items\"][i][\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textOriginal\"])\n",
        "  df = pd.DataFrame(comments, index = authorname,columns=[\"Comments\"])\n",
        "\n",
        "  df = add_polarity(df)\n",
        "  if training_data:\n",
        "      return df\n",
        "\n",
        "  df.to_csv(f\"video_comments_{video_id}.csv\")\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "r9AJey_Rcva9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_comments(video_id, training_data = False):\n",
        "    name = \"video_comments_\" + video_id + \".csv\"\n",
        "    if name in os.listdir():\n",
        "        print(\"You are using video that you previously passed, fetching previous comments.\")\n",
        "        df = pd.read_csv(name)\n",
        "        return df\n",
        "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
        "    api_service_name = \"youtube\"\n",
        "    api_version = \"v3\"\n",
        "    API_KEY = \"API_KEY\"\n",
        "\n",
        "    youtube = googleapiclient.discovery.build(api_service_name, api_version, developerKey = API_KEY)\n",
        "\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"id, snippet\",\n",
        "        maxResults=1000,\n",
        "        order=\"relevance\",\n",
        "        videoId= video_id\n",
        "    )\n",
        "    response = request.execute()\n",
        "    \n",
        "    return create_dataframe(response, video_id, training_data)"
      ],
      "metadata": {
        "id": "MvkQHku5YJvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model):\n",
        "    with open('youtube_comment_analyzer.pkl', 'wb') as model_file:\n",
        "        pickle.dump(model, model_file)"
      ],
      "metadata": {
        "id": "701hzmBnazoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_video_ids(count = 10):\n",
        "    # import random\n",
        "    # API_KEY = 'AIzaSyByf6YnjVaPP1FWII2lO0AnRrzgT9X_Uk8'\n",
        "    # random = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(3))\n",
        "    # urlData = \"https://www.googleapis.com/youtube/v3/search?key={}&maxResults={}&part=snippet&type=video&q={}\".format(API_KEY,count,random)\n",
        "    # webURL = urllib.request.urlopen(urlData)\n",
        "    # data = webURL.read()\n",
        "    # encoding = webURL.info().get_content_charset('utf-8')\n",
        "    # results = json.loads(data.decode(encoding))\n",
        "    \n",
        "    video_ids = [\"09k7EUnx5sE\", \"rW5zJgsZZuk\", \"gAfYT6Qz_14\", \"HwLK9dBQn0g\", \"HgiiY9TLtX8\", \"eeHLyNFOXM4\", \"oSLz-iw_Oy4\", \"AC908sfmPao\", \"lJCUC0mRkPo\", \"lkDBImBAmN0\"]\n",
        "\n",
        "    # for data in results['items']:\n",
        "    #     videoId = (data['id']['videoId'])\n",
        "    #     video_ids.append(videoId)\n",
        "    return video_ids"
      ],
      "metadata": {
        "id": "jsnplCBqKHWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_comments(video_ids):\n",
        "    comments = pd.DataFrame(columns = ['Comments', 'stop_comments', 'polarity', 'pol_cat'])\n",
        "\n",
        "    for video_id in video_ids:\n",
        "        comments = pd.concat([comments, get_comments(video_id, training_data = True)])\n",
        "    comments.to_csv(\"comments.csv\")\n",
        "    print(\"File saved successfully as comments.csv\")"
      ],
      "metadata": {
        "id": "TCFq64tRRWyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    video_ids = get_random_video_ids(10)\n",
        "    comments = \"comments.csv\"\n",
        "    if comments not in os.listdir():\n",
        "        print(\"Comments.csv not found.\")\n",
        "        print(\"Fetching random comments for training model.\")\n",
        "        comments = get_all_comments(video_ids)\n",
        "    else:\n",
        "        print(\"Comments.csv found\")\n",
        "        print(\"Reading comments.csv please wait!\")\n",
        "        comments = pd.read_csv(comments)\n",
        "# create_model()\n",
        "\n",
        "    X_train,X_test,y_train,y_test = train_test_split(comments['stop_comments'], comments['pol_cat'], test_size = 0.2)\n",
        "    # vect = CountVectorizer()\n",
        "    tf_train = vect.fit_transform(X_train)\n",
        "    tf_test = vect.transform(X_test)\n",
        "    lr = LogisticRegression()\n",
        "    lr.fit(tf_train,y_train)\n",
        "    print(\"Accuracy: \", int(lr.score(tf_train, y_train)*100), \"%\")\n",
        "    save_model(lr)\n",
        "    return"
      ],
      "metadata": {
        "id": "G_KVTYRRZMpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_video(video_link=None):\n",
        "    clear = lambda: os.system('cls')\n",
        "    clear()\n",
        "    if video_link is None:\n",
        "        print(\"Please provide a valid video Id.\")\n",
        "        print(\"Exiting ....\")\n",
        "        return None\n",
        "    video_id = get_videoId(video_link)\n",
        "    if video_id is None:\n",
        "        return None\n",
        "    \n",
        "    comments = get_comments(video_id)\n",
        "\n",
        "    model = \"youtube_comment_analyzer.pkl\"\n",
        "    if model not in os.listdir():\n",
        "        print(\"Model is not found. Creating a new model. Please wait ...\")\n",
        "        create_model()\n",
        "        print(\"Model created successully.\")\n",
        "    else:\n",
        "        print(\"Found existing model, trying to work on that.\")\n",
        "\n",
        "    with open(model , 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    # vect = CountVectorizer()\n",
        "    v = vect.transform(comments[\"stop_comments\"])\n",
        "    predicted = model.predict(v)\n",
        "\n",
        "    count_positive = list(predicted).count(1)\n",
        "    count_negative = list(predicted).count(-1)\n",
        "\n",
        "    review = \"\"\n",
        "    if count_positive > count_negative:\n",
        "        review = \"This youtube video have positive reviews\"\n",
        "    elif count_positive == count_negative:\n",
        "        review = \"This youtube video has average reviews. It is nither bad not good\"\n",
        "    else:\n",
        "        review = \"This youtube video have negative reviews\"\n",
        "    \n",
        "    total = count_positive + count_negative\n",
        "    star_value = total // 5\n",
        "\n",
        "    if count_positive > star_value and count_positive < (2*star_value):\n",
        "        print(review + \" \" + \"1/5 star.\")\n",
        "    elif count_positive > (2*star_value) and count_positive < (3*star_value):\n",
        "        print(review + \" \" + \"2/5 star.\")\n",
        "    elif count_positive > (3*star_value) and count_positive < (4*star_value):\n",
        "        print(review + \" \" + \"3/5 star.\")\n",
        "    elif count_positive > (4*star_value) and count_positive < (5*star_value):\n",
        "        print(review + \" \" + \"4/5 star.\")\n",
        "    else:\n",
        "        print(review + \" \" + \"5/5 star.\")\n",
        "    # print(predicted)"
      ],
      "metadata": {
        "id": "DVICoN5CaJRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_video(\"https://www.youtube.com/watch?v=09k7EUnx5sE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVeTIQUsP654",
        "outputId": "50d328f8-ae26-4f6b-de8c-dff2d7203db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are using video that you previously passed, fetching previous comments.\n",
            "Model is not found. Creating a new model. Please wait ...\n",
            "Comments.csv found\n",
            "Reading comments.csv please wait!\n",
            "Accuracy:  98 %\n",
            "Model created successully.\n",
            "This youtube video have positive reviews 4/5 star.\n"
          ]
        }
      ]
    }
  ]
}